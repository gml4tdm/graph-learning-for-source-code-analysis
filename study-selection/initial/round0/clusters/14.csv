Title,Abstract
A speaker-aware multiparty dialogue discourse parser with heterogeneous graph neural network,"Discourse parsing for multiparty dialogue aims to detect the discourse structure and relations in a dialogue to obtain a discourse dependency graph. Existing models for this task have proven the effect of speaker information. However, no model explicitly learns representations of the speaker for parsing the discourse structure of dialogues. In this paper, to further exploit the effect of speaker information, we propose a novel model HG-MDP, which uses a heterogeneous graph neural network to encode dialogue graphs and we use an iterative update for the aggregation of speaker nodes and utterance nodes. Finally, we adopt updated utterance nodes to predict discourse dependency links and relations using the biaffine module. To validate our HG-MDP model, we perform experiments on the two existing benchmarks STAC and Molweni corpus. The results prove the effectiveness of the speaker modeling module on two datasets and we achieve the state-of-the-art on the Molweni dataset. © 2023 Elsevier B.V."
Enhancing Table Retrieval with Dual Graph Representations,"Table retrieval aims to rank candidate tables for answering natural language query, in which the most critical problem is how to learn informative representations for structured tables. Most previous methods roughly flatten the table and send it into a sequence encoder, ignoring the structure information of tables and the semantic interaction between table cells and contexts. In this paper, we propose a dual graph based method to perceive the semantics and structure of tables, so as to preferably support the downstream table retrieval task. Inspired by human cognition, we first decouple a table into the row view and column view, then build dual graphs from these two views with the consideration of table contexts. Afterward, intra-graph and inter-graph interactions are iteratively performed for aggregating and exchanging local row- and column-oriented features respectively, and an adaptive fusion strategy is eventually tailor-made for sophisticated table representations. In this way, the table structure and semantic information are well considered with dual-graph modeling. Consequently, the input query can match the target tables based on their full-fledged table representations and achieve the ultimate ranking results more accurately. Extensive experiments verify the superiority of our dual graphs over strong baselines on two table retrieval datasets WikiTables and WebQueryTable. Further analyses also confirm the adaptability for row-/column-oriented tables, and show the rationality and generalization of dual graphs. The source code is available at https://github.com/ty33123/DualG. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG."
Context-aware Inductive Graph Matrix Completion with Sentence BERT,"Existing graph neural network (GNN) based recommendation models depend highly on initial node features and graph structures. Most prior studies do not support the use of additional information on user-item interactions and adopt a transductive method, which has limitations for actual web service. To overcome these problems, we propose Context-aware Inductive Graph Matrix Completion (CGMC), which utilizes context vectors that represent user-item interactions and user-item bipartite graph structures in an inductive manner. Our method constructs context vectors from the review, timestamp, and rating and uses them as edge features of the user-item bipartite graph. Relations between homogeneous nodes are also constructed based on common neighbors. Our model combines context vectors and relational information using Context-aware Graph Attention Networks and Edge Fusion Graph Convolutional Networks. We conducted extensive experiments using six real-world datasets, and the results show that the proposed model achieves superior performances over other competing models. Furthermore, we analyzed the inductive characteristics of CGMC through the cross-domain transferability. The source code is available in the repository at [https://github.com/venzino-han/CGMC_SBERT]. © 2023 IEEE."
VTCC-NLP at SemEval-2023 Task 6: Long-Text Representation Based on Graph Neural Network for Rhetorical Roles Prediction,"Rhetorical Roles (RR) prediction is to predict the label of each sentence in legal documents, which is regarded as an emergent task for legal document understanding. In this study, we present a novel method for the RR task by exploiting the long context representation. Specifically, legal documents are known as long texts, in which previous works have no ability to consider the inherent dependencies among sentences. In this paper, we propose GNNRR (Graph Neural Network for Rhetorical Roles Prediction), which is able to model the cross-information for long texts. Furthermore, we develop multitask learning by incorporating label shift prediction (LSP) for segmenting a legal document. The proposed model is evaluated on the SemEval 2023 Task 6 - Legal Eval Understanding Legal Texts for RR sub-task. Accordingly, our method achieves the top 4 in the public leaderboard of the sub-task. Our source code is available for further investigation1 © 2023 Association for Computational Linguistics."
Discriminative Graph-Level Anomaly Detection via Dual-Students-Teacher Model,"Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing student models trained by normal and abnormal graphs respectively fit graph representations of the teacher model in terms of node-level and graph-level representation perspectives. Finally, we combine representation errors between two student models to discriminatively distinguish anomalous graphs. Extensive experiment analysis demonstrates that our method is effective for the graph-level anomaly detection task on graph datasets in the real world.(The source code is at https://github.com/whb605/GLADST.git ). © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023."
Biomedical Document Classification with Literature Graph Representations of Bibliographies and Entities,"This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information. Recently, document classification performance has been significantly improved with large pre-trained language models; however, there still remain documents that are difficult to classify. External information, such as bibliographic information, citation links, descriptions of entities, and medical taxonomies, has been considered one of the keys to dealing with such documents in document classification. Although several document classification methods using external information have been proposed, they only consider limited relationships, e.g., word co-occurrence and citation relationships. However, there are multiple types of external information. To overcome the limitation of the conventional use of external information, we propose a document classification model that simultaneously considers bibliographic and entity information to deeply model the relationships among documents using the representations of the literature graph. The experimental results show that our proposed method outperforms existing methods on two document classification datasets in the biomedical domain with the help of the literature graph. Our source code is publicly available at https://github.com/tticoin/BDCL-LitGraph. © 2023 Association for Computational Linguistics."
Integrating Heterogeneous Graphs Using Graph Transformer Encoder for Solving Math Word Problems,"This paper introduces a novel method that integrates structural information with training deep neural models to solve math word problems. Prior works adopt the graph structure to represent rich information residing in the input sentences. However, they lack the consideration of different relation types between other parts of the sentences. To provide various types of structural information in a uniform way, we propose a graph transformer encoder to integrate heterogeneous graphs of various input representations. We developed two types of graph structures. First, the Dependency Graph maintains long-distance lexical dependency between words and quantities. Second, the Question Overlap Graph captures the gist within the problem body. The two graphs are encoded as a single graph for graph transformation. Experimental results show that our method produces competitive results compared to the baselines. Our model outperforms state-of-the-art models in Equation and Answer accuracy near three percent in SVAMP benchmark. Moreover, we discuss that integrating different types of textual characteristics may improve the quality of mathematical logic inference from natural language sentences. © 2013 IEEE."
HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization,"Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically, in the case of the Extractive Document Summarization (EDS) task, modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g., word-to-word, word-to-sentence, sentence-to-sentence) and enrich valuable information for the sentence representation. However, long-form document summarization using graph-based approaches is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard, this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically, the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard, HeterGraphLongSum includes three types of semantic units such as word, sentence, and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially, HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT). The source code is available for further exploitation on the Github © 2022 Proceedings - International Conference on Computational Linguistics, COLING. All rights reserved."
Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification,"Recently, graph neural networks (GNNs) have been widely used for document classification. However, most existing methods are based on static word co-occurrence graphs without sentence-level information, which poses three challenges:( 1) word ambiguity, (2) word synonymity, and (3) dynamic contextual dependency. To address these challenges, we propose a novel GNN-based sparse structure learning model for inductive document classification. Specifically, a document-level graph is initially generated by a disjoint union of sentence-level word co-occurrence graphs. Our model collects a set of trainable edges connecting disjoint words between sentences, and employs structure learning to sparsely select edges with dynamic contextual dependencies. Graphs with sparse structures can jointly exploit local and global contextual information in documents through GNNs. For inductive learning, the refined document graph is further fed into a general readout function for graph-level classification and optimization in an end-to-end manner. Extensive experiments on several real-world datasets demonstrate that the proposed model outperforms most state-of-the-art results, and reveal the necessity to learn sparse structures for each document.  Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
Scalable Motif Counting for Large-scale Temporal Graphs,"One fundamental problem in temporal graph anal-ysis is to count the occurrences of small connected subgraph patterns (i.e., motifs), which benefits a broad range of real-world applications, such as anomaly detection, structure prediction, and network representation learning. However, existing works focused on exacting temporal motif are not scalable to large-scale temporal graph data, due to their heavy computational costs or inherent inadequacy of parallelism. In this work, we propose a scalable parallel framework for exactly counting temporal motifs in large-scale temporal graphs. We first categorize the temporal motifs based on their distinct properties, and then design customized algorithms that offer efficient strategies to exactly count the motif instances of each category. Moreover, our compact data structures, namely triple and quadruple counters, enable our algorithms to directly identify the temporal motif instances of each category, according to edge information and relationship between edges, therefore significantly improving the counting efficiency. Based on the proposed counting algorithms, we design a hierarchical parallel framework that featuring both inter- and intra-node parallel strategies, and fully leverages the multi-threading capacity of modern CPU to concurrently count all temporal motifs. Extensive experiments on sixteen real-world temporal graph datasets demonstrate the superiority and capability of our proposed framework for temporal motif counting, achieving up to 538× speedup compared to the state-of-the-art methods. The source code of our method is available at: https://github.com/steven-ccq/FAST-temporal-motif.  © 2022 IEEE."
Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification,"Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels. © 2021 Association for Computational Linguistics"
Modular Graph Transformer Networks for Multi-Label Image Classification,"With the recent advances in graph neural networks, there is a rising number of studies on graph-based multi-label classification with the consideration of object dependencies within visual data. Nevertheless, graph representations can become indistinguishable due to the complex nature of label relationships. We propose a multi-label image classification framework based on graph transformer networks to fully exploit inter-label interactions. The paper presents a modular learning scheme to enhance the classification performance by segregating the computational graph into multiple sub-graphs based on modularity. Our approach, named Modular Graph Transformer Networks (MGTN), is capable of employing multiple backbones for better information propagation over different sub-graphs guided by graph transformers and convolutions. We validate our framework on MS-COCO and Fashion550K datasets to demonstrate improvements for multilabel image classification. The source code is available at https://github.com/ReML-AI/MGTN. © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding,"To capture higher-order structural features, most GNN-based algorithms learn node representations incorporating k-hop neighbors' information. Due to the high time complexity of querying k-hop neighbors, most graph algorithms cannot be deployed in a giant dense temporal network to execute millisecond-level inference. This problem dramatically limits the potential of applying graph algorithms in certain areas, especially financial fraud detection. Therefore, we propose Asynchronous Propagation Attention Network, an asynchronous continuous time dynamic graph algorithm for real-time temporal graph embedding. Traditional graph models usually execute two serial operations: first graph querying and then model inference. Different from previous graph algorithms, we decouple model inference and graph computation to alleviate the damage of the heavy graph query operation to the speed of model inference. Extensive experiments demonstrate that the proposed method can achieve competitive performance while greatly improving the inference speed. The source code is published at a Github repository. © 2021 ACM."
DAGN: Discourse-Aware Graph Network for Logical Reasoning,"Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/EleanorH/DAGN. © 2021 Association for Computational Linguistics."
Corpus-Aware Graph Aggregation Network for Sequence Labeling,"Current state-of-the-art sequence labeling models are typically based on sequential architecture such as Bi-directional LSTM (BiLSTM). However, the structure of processing a word at a time based on the sequential order restricts the full utilization of non-sequential features, including syntactic relationships, word co-occurrence relations, and document topics. They can be regarded as the corpus-level features and critical for sequence labeling. In this paper, we propose a Corpus-Aware Graph Aggregation Network. Specifically, we build three types of graphs, i.e., a word-topic graph, a word co-occurrence graph, and a word syntactic dependency graph, to express different kinds of corpus-level non-sequential features. After that, a graph convolutional network (GCN) is adapted to model the relations between words and non-sequential features. Finally, we employ a label-aware attention mechanism to aggregate corpus-aware non-sequential features and sequential ones for sequence labeling. The experimental results on four sequence labeling tasks (named entity recognition, chunking, multilingual sequence labeling, and target-based sentiment analysis) show that our model achieves state-of-the-art performance.  © 2014 IEEE."
Product Graph Learning from Multi-Domain Data with Sparsity and Rank Constraints,"In this paper, we focus on learning product graphs from multi-domain data. We assume that the product graph is formed by the Cartesian product of two smaller graphs, which we refer to as graph factors. We pose the product graph learning problem as the problem of estimating the graph factor Laplacian matrices. To capture local interactions in data, we seek sparse graph factors and assume a smoothness model for data. We propose an efficient iterative solver for learning sparse product graphs from data. We then extend this solver to infer multi-component graph factors with applications to product graph clustering by imposing rank constraints on the graph Laplacian matrices. Although working with smaller graph factors is computationally more attractive, not all graphs readily admit an exact Cartesian product factorization. To this end, we propose efficient algorithms to approximate a graph by a nearest Cartesian product of two smaller graphs. The efficacy of the developed framework is demonstrated using several numerical experiments on synthetic and real data.  © 1991-2012 IEEE."
G3raphGround: Graph-based language grounding,"In this paper we present an end-to-end framework for grounding of phrases in images. In contrast to previous works, our model, which we call GraphGround, uses graphs to formulate more complex, non-sequential dependencies among proposal image regions and phrases. We capture intra-modal dependencies using a separate graph neural network for each modality (visual and lingual), and then use conditional message-passing in another graph neural network to fuse their outputs and capture cross-modal relationships. This final representation results in grounding decisions. The framework supports many-to-many matching and is able to ground single phrase to multiple image regions and vice versa. We validate our design choices through a series of ablation studies and illustrate state-of-the-art performance on Flickr30k and ReferIt Game benchmark datasets. © 2019 IEEE."
"Double bounded rough set, tension measure, and social link prediction","This paper describes a new approach of viewing a social relation as a string with various forces acting on it. Accordingly, a tension measure for a relation is defined. Various component forces of the tension measure are identified based on the structural information of the network. A new variant of rough set, namely, double bounded rough set, is developed in order to define these forces mathematically. It is revealed experimentally with synthetic and real-world data that positive and negative tension characterizes, relatively, the presence and absence of a physical link between two nodes. An algorithm based on tension measure is proposed for link prediction. Superiority of the algorithm is demonstrated on nine real-world networks, which include four temporal networks. The source code for calculating tension measure and link prediction algorithm is publicly available at https://gitlab.com/suman5/social-tension-measure. © 2014 IEEE."
The pyramid quantized Weisfeiler-Lehman graph representation,"Graphs are flexible and powerful representations for non-vectorial structured data. Graph kernels have been shown to enable efficient and accurate statistical learning on this important domain, but many graph kernel algorithms have high order polynomial time complexity. Efficient graph kernels rely on a discrete node labeling as a central assumption. However, many real world domains are naturally described by continuous or vector valued node labels. In this paper, we propose an efficient graph representation and comparison scheme for large graphs with continuous vector labels, the pyramid quantized Weisfeiler-Lehman graph representation. Our algorithm considers statistics of subtree patterns with discrete labels based on the Weisfeiler-Lehman algorithm and uses a pyramid quantization strategy to determine a logarithmic number of discrete labelings that results in a representation that guarantees a multiplicative error bound on an approximation to the optimal partial matching. As a result, we approximate a graph representation with continuous vector labels as a sequence of graphs with increasingly granular discrete labels. We evaluate our proposed algorithm on two different tasks with real datasets, on a fMRI analysis task and on the generic problem of 3D shape classification. Source code of the implementation can be downloaded from https://web.imis.athena-innovation.gr/~kgkirtzou/Projects/WLpyramid.html. © 2015 Elsevier B.V.."
