Title,Abstract
Data-Driven Self-Supervised Graph Representation Learning,"Self-supervised graph representation learning (SSGRL) is a representation learning paradigm used to reduce or avoid manual labeling. An essential part of SSGRL is graph data augmentation. Existing methods usually rely on heuristics commonly identified through trial and error and are effective only within some application domains. Also, it is not clear why one heuristic is better than another. Moreover, recent studies have argued against some techniques (e.g., dropout: that can change the properties of molecular graphs or destroy relevant signals for graph-based document classification tasks). In this study, we propose a novel data-driven SSGRL approach that automatically learns a suitable graph augmentation from the signal encoded in the graph (i.e., the nodes' predictive feature and topological information). We propose two complementary approaches that produce learnable feature and topological augmentations. The former learns multi-view augmentation of node features, and the latter learns a high-order view of the topology. Moreover, the augmentations are jointly learned with the representation. Our approach is general that it can be applied to homogeneous and heterogeneous graphs. We perform extensive experiments on node classification (using nine homogeneous and heterogeneous datasets) and graph property prediction (using another eight datasets). The results show that the proposed method matches or outperforms the SOTA SSGRL baselines and performs similarly to semi-supervised methods. The anonymised source code is available at https://github.com/AhmedESamy/dsgrl/ © 2023 The Authors."
Tensorized topological graph learning for generalized incomplete multi-view clustering,"The success of the current multi-view clustering lies in the default assumption of completeness on each view, while it is hardly satisfied for real-world applications. Incomplete multi-view clustering (IMC) studies clustering multi-view instances that are partially observed due to data corruption or sensor failure. Although making some progress, the current research suffers from the following obstacles: (1) Existing works are always built on pairwise similarity measurement, which cannot fully capture the topological structure of incomplete multi-view samples for precise clustering; (2) Due to the absence of partially-observed samples across multiple views, how could we recover and enhance the high-order similarities across different views becomes one of the main bottlenecks of IMC; (3) Current research mainly aims to handle specific dual-view IMC, which greatly limits the deployment in real-world applications. In this paper, we propose a novel Tensorized Topological Graph Learning (TTGL) for the generalized incomplete multi-view clustering, which jointly considers the topological graph construction, missing feature completion, and high-order uncertain correlation enhancement. Specifically, instead of using pairwise similarity measurement, we formulate a consensus topological graph construction module, which can coalesce affinity matrices to analyze the topological structure of incomplete multi-view data. Moreover, we build an iterative missing similarity imputation scheme for feature completion, which ensures the intrinsic similarity preservation on observed instances as well as an imputation on unobserved ones. Furthermore, a low-rank tensor structure is imposed to capture the high-order similarities via both feature and similarity enhancement and completion across different views. Our method can cluster incomplete data with an arbitrary number of views and any missing statuses. Extensive experiments on several benchmark datasets validate the effectiveness of our method when compared with a series of state-of-the-art IMC clustering algorithms. The source code of our TTGL is available at: https://github.com/DarrenZZhang/TTGL. © 2023 Elsevier B.V."
Self-supervised contrastive graph representation with node and graph augmentation,"Graph representation is a critical technology in the field of knowledge engineering and knowledge-based applications since most knowledge bases are represented in the graph structure. Nowadays, contrastive learning has become a prominent way for graph representation by contrasting positive–positive and positive–negative node pairs between two augmentation graphs. It has achieved new state-of-the-art in the field of self-supervised graph representation. However, existing contrastive graph representation methods mainly focus on modifying (normally removing some edges/nodes) the original graph structure to generate the augmentation graph for the contrastive. It inevitably changes the original graph structures, meaning the generated augmentation graph is no longer equivalent to the original graph. This harms the performance of the representation in many structure-sensitive graphs such as protein graphs, chemical graphs, molecular graphs, etc. Moreover, there is only one positive–positive node pair but relatively massive positive–negative node pairs in the self-supervised graph contrastive learning. This can lead to the same class, or very similar samples are considered negative samples. To this end, in this work, we propose a Virtual Masking Augmentation (VMA) to generate an augmentation graph without changing any structures from the original graph. Meanwhile, a node augmentation method is proposed to augment the positive node pairs by discovering the most similar nodes in the same graph. Then, two different augmentation graphs are generated and put into a contrastive learning model to learn the graph representation. Extensive experiments on massive datasets demonstrate that our method achieves new state-of-the-art results on self-supervised graph representation. The source code of the proposed method is available at https://github.com/DuanhaoranCC/CGRA. © 2023 The Author(s)"
Entropy Neural Estimation for Graph Contrastive Learning,"Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. We theoretically illustrate that the entropy of a dataset is approximated by maximizing the lower bound of the mutual information across different views of a graph, i.e., entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. We conduct experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code is available at https://github.com/kunzhan/M-ILBO. © 2023 ACM."
Multiple kernel graph clustering with shifted Laplacian reconstruction,"Multiple kernel graph clustering methods have gained favor among researchers for their ability to combine the strengths of graph learning and multiple kernel methods. We propose a novel method named multiple kernel graph clustering with shifted Laplacian reconstruction (SLR-MKGC). Specifically, the kernel matrix is regarded as an affinity graph, and then the graph is transformed to a shifted Laplacian matrix. By decomposing the shifted Laplacian matrix, the latent data representation is obtained, simultaneously preserving the main energy and clustering information. As a result, the effects of noise and redundancy are reduced and the quality of the raw data are improved. Then, the obtained representation is used to reconstruct a final affinity graph with a desired block diagonal structure. Further, we conduct extensive experiments on seven benchmark datasets and compare nine state-of-the-art clustering methods. Experimental results show that SLR-MKGC exhibits excellent performance on most datasets. For example, compared to other state-of-the-art methods, SLR-MKGC achieves a performance improvement of 5.13% on the clustering accuracy term on the BBCSport2 dataset, demonstrating the promise of SLR-MKGC. The source code is available at https://github.com/dililidida/SLR-MKGC. © 2023 Elsevier Ltd"
Self-supervised contrastive learning on heterogeneous graphs with mutual constraints of structure and feature,"Self-supervised learning on heterogeneous graphs has gained significant attention as it eliminates the need for manual labeling. However, most existing researches focus on predefined meta-paths that relies on domain knowledge, and they cannot handle the noises in graphs effectively. To address these problems, we propose a self-supervised contrastive learning method on heterogeneous graphs with mutual constraints of structure and feature called HeMuc. Specifically, in the high-order relation view, we exploit graph reachability to obtain the sequence of target nodes traversed by source nodes. Furthermore, we design degree and feature constraints to reduce the noises in topological structure. In the feature view, we reconstruct the graph structure using the similarity between node features and eliminate the dependence on the original graph. Finally, we propose a contrastive learning method by designing a new sampling strategy that combines the structure and feature information. The experimental results on the tasks of node classification and node clustering demonstrate that the proposed HeMuc outperforms the state-of-the-art methods. The source codes of this work are available at https://github.com/ZZY-GraphMiningLab/HeMuc. © 2023 Elsevier Inc."
Graph Self-Supervised Learning: A Survey,"Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further describe the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.  © 1989-2012 IEEE."
Contrastive Cross-scale Graph Knowledge Synergy,"Graph representation learning via Contrastive Learning (GCL) has drawn considerable attention recently. Efforts are mainly focused on gathering more global information via contrasting on a single high-level graph view, which, however, underestimates the inherent complex and hierarchical properties in many real-world networks, leading to sub-optimal embeddings. To incorporate these properties of a complex graph, we propose Cross-Scale Contrastive Graph Knowledge Synergy (CGKS), a generic feature learning framework, to advance graph contrastive learning with enhanced generalization ability and the awareness of latent anatomies. Specifically, to maintain the hierarchical information, we create a so-call graph pyramid (GP) consisting of coarse-grained graph views. Each graph view is obtained via the careful design topology-aware graph coarsening layer that extends the Laplacian Eigenmaps with negative sampling. To promote cross-scale information sharing and knowledge interactions among GP, we propose a novel joint optimization formula that contains a pairwise contrastive loss between any two coarse-grained graph views. This synergy loss not only promotes knowledge sharing that yields informative representations, but also stabilizes the training process. Experiments on various downstream tasks demonstrate the substantial improvements of the proposed method over its counterparts.  © 2023 ACM."
Let the Data Choose: Flexible and Diverse Anchor Graph Fusion for Scalable Multi-View Clustering,"In the past few years, numerous multi-view graph clustering algorithms have been proposed to enhance the clustering performance by exploring information from multiple views. Despite the superior performance, the high time and space expenditures limit their scalability. Accordingly, anchor graph learning has been introduced to alleviate the computational complexity. However, existing approaches can be further improved by the following considerations: (i) Existing anchor-based methods share the same number of anchors across views. This strategy violates the diversity and flexibility of multi-view data distribution. (ii) Searching for the optimal anchor number within hyper-parameters takes much extra tuning time, which makes existing methods impractical. (iii) How to flexibly fuse multi-view anchor graphs of diverse sizes has not been well explored in existing literature. To address the above issues, we propose a novel anchor-based method termed Flexible and Diverse Anchor Graph Fusion for Scalable Multi-view Clustering (FDAGF) in this paper. Instead of manually tuning optimal anchor with massive hyper-parameters, we propose to optimize the contribution weights of a group of pre-defined anchor numbers to avoid extra time expenditure among views. Most importantly, we propose a novel hybrid fusion strategy for multi-size anchor graphs with theoretical proof, which allows flexible and diverse anchor graph fusion. Then, an efficient linear optimization algorithm is proposed to solve the resultant problem. Comprehensive experimental results demonstrate the effectiveness and efficiency of our proposed framework. The source code is available at https://github.com/Jeaninezpp/FDAGF. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
Inclusivity induced adaptive graph learning for multi-view clustering,"Graph-based multi-view clustering, with its ability to mine potential associations between data samples, has attracted extensive attention. However, existing methods directly learn affinity graphs from multiple feature views without removing noisy information mixed with the original data, resulting in limited performance. In addition, only shared information is included in the graph fusion process, which excludes inconsistent information, resulting in insufficient exploration of complementary information among multi-view data. In this study, we propose an inclusivity-induced adaptive graph learning (IiAGL) method to address these issues. First, inclusivity is defined as the capability of a view to accommodate other views. Specifically, we extended subspace learning to graph construction, enabling view-specific graphs to capture the underlying data distribution. Furthermore, an inclusivity-induced regularizer is introduced to guide graph rebuilding and graph fusion, and the entire process is performed using a self-weighted strategy. In this manner, we can obtain a clean consensus containing shared representations and complementary information from different views. An alternating direction method with augmented Lagrangian multiplier (ADM-ALM) was designed to solve the resulting optimization problem. Extensive experiments on diverse datasets demonstrated that the clustering performance of the proposed method outperformed that of other state-of-the-art methods. The source code for this study is publicly available at https://github.com/ryan-xinzou/IiAGL-MC. © 2023 Elsevier B.V."
Adversarial Graph Disentanglement With Component-Specific Aggregation,"A real-world graph has a complex topological structure, which is often formed by the interaction of different latent factors. Disentanglement of these latent factors can effectively improve the robustness and expressiveness of the node representation of a graph. However, most existing methods lack consideration of the intrinsic differences in relations between nodes caused by factor entanglement. In this paper, we propose an <underline><bold>A</bold></underline>dversarial <underline><bold>D</bold></underline>isentangled <underline><bold>G</bold></underline>raph <underline><bold>C</bold></underline>onvolutional <underline><bold>N</bold></underline>etwork (ADGCN) for disentangled graph representation learning. To begin with, we point out two aspects of graph disentanglement that need to be considered, i.e., micro-disentanglement and macro-disentanglement. For them, a component-specific aggregation approach is proposed to achieve micro-disentanglement by inferring latent components that caused the links between nodes. On the basis of micro-disentanglement, we further propose a macro-disentanglement adversarial regularizer to improve the separability among component distributions, thus restricting the interdependence among components. Additionally, to reveal the topological graph structure, a diversity-preserving node sampling approach is proposed, by which the graph structure can be progressively refined in a way of local structure awareness. The experimental results on various real-world graph data verify that our ADGCN obtains more favorable performance over currently available alternatives. The source codes of ADGCN are available at <uri>https://github.com/SsGood/ADGCN</uri>. IEEE"
Shared-Attribute Multi-Graph Clustering with Global Self-Attention,"Recently, multi-view attributed graph clustering has attracted lots of attention with the explosion of graph-structured data. Existing methods are primarily designed for the form in which every graph has its attributes. We argue that a more natural form of multi-view attributed graph data contains shared node attributes and multiple graphs, which we called “multi-graph”. When simply applying existing methods to multi-graph clustering, the information of shared attributes is not well exploited to eliminate the large variances among different graphs. Therefore, we propose a Shared-Attribute Multi-Graph Clustering with global self-attention (SAMGC) method for multi-graph clustering. The main ideas of SAMGC are: 1) Global self-attention is proposed to construct the supplementary graph from shared attributes for each graph. 2) Layer attention is proposed to meet the requirements for different layers in different graphs. 3) A novel self-supervised weighting strategy is proposed to de-emphasize unimportant graphs. Our experiments on four benchmark datasets show the superiority of SAMGC over 14 SOTA methods. The source code is available at https://github.com/cjpcool/SAMGC. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG."
Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations,"Graph contrastive learning (GCL) is a promising direction toward alleviating the label dependence, poor generalization and weak robustness of graph neural networks, learning representations with invariance, and discriminability by solving pretasks. The pretasks are mainly built on mutual information estimation, which requires data augmentation to construct positive samples with similar semantics to learn invariant signals and negative samples with dissimilar semantics to empower representation discriminability. However, an appropriate data augmentation configuration depends heavily on lots of empirical trials such as choosing the compositions of data augmentation techniques and the corresponding hyperparameter settings. We propose an augmentation-free GCL method, invariant-discriminative GCL (iGCL), that does not intrinsically require negative samples. iGCL designs the invariant-discriminative loss (ID loss) to learn invariant and discriminative representations. On the one hand, ID loss learns invariant signals by directly minimizing the mean square error (MSE) between the target samples and positive samples in the representation space. On the other hand, ID loss ensures that the representations are discriminative by an orthonormal constraint forcing the different dimensions of representations to be independent of each other. This prevents representations from collapsing to a point or subspace. Our theoretical analysis explains the effectiveness of ID loss from the perspectives of the redundancy reduction criterion, canonical correlation analysis (CCA), and information bottleneck (IB) principle. The experimental results demonstrate that iGCL outperforms all baselines on five node classification benchmark datasets. iGCL also shows superior performance for different label ratios and is capable of resisting graph attacks, which indicates that iGCL has excellent generalization and robustness. The source code is available at https://github.com/lehaifeng/ T-GCN/tree/master/iGCL. IEEE"
Fast Unsupervised Graph Embedding via Graph Zoom Learning,"Unsupervised graph representation learning, i.e., learning node or graph embeddings from graph data in an unsupervised manner, has become an important problem when we study graph data. With the development of self-supervised learning, researchers have designed graph-level self-supervised learning paradigms and learn embeddings under these paradigms. The learned embeddings can serve as a fine initial solution to downstream tasks such as node classification or graph classification. In this paper, we propose a fast unsupervised graph embedding method, which follows the way of self-supervised learning. This method performs representation learning on the graph under a novel concept called Graph Zoom Learning (abbr. GZL), which is orthogonal to the existing concepts of unsupervised graph embedding, such as random walk and contrastive learning. Two crucial components, graph zoom-out and point-to-point contrast, help GZL reduce the overall training time cost. Specifically, on the one hand, a lightweight miniature graph is generated from the raw graph by graph zoom-out and the learning on the miniature graph is more efficient than the learning on the raw graph; on the other hand, we design the miniature-scale learning on the miniature graph and introduce community structure into this learning pattern, which contributes to the final point-to-point contrast. Since point-to-point contrast is independent of negatives, it makes the whole training more efficient. We conduct extensive experiments to verify the advantage of GZL on representation learning. On two downstream tasks of node classification and graph classification, GZL outperforms the state-of-the-art unsupervised graph embedding methods. Particularly, on the largest experimental graph dataset (ogbn-arxiv) with 169k nodes and 1.1m edges, GZL outperforms the runner-up by 3.3% relative accuracy and achieves up to 22.6x speedup over it.  © 2023 IEEE."
Hierarchically Contrastive Hard Sample Mining for Graph Self-Supervised Pretraining,"Contrastive learning has recently emerged as a powerful technique for graph self-supervised pretraining (GSP). By maximizing the mutual information (MI) between a positive sample pair, the network is forced to extract discriminative information from graphs to generate high-quality sample representations. However, we observe that, in the process of MI maximization (Infomax), the existing contrastive GSP algorithms suffer from at least one of the following problems: 1) treat all samples equally during optimization and 2) fall into a single contrasting pattern within the graph. Consequently, the vast number of well-categorized samples overwhelms the representation learning process, and limited information is accumulated, thus deteriorating the learning capability of the network. To solve these issues, in this article, by fusing the information from different views and conducting hard sample mining in a hierarchically contrastive manner, we propose a novel GSP algorithm called hierarchically contrastive hard sample mining (HCHSM). The hierarchical property of this algorithm is manifested in two aspects. First, according to the results of multilevel MI estimation in different views, the MI-based hard sample selection (MHSS) module keeps filtering the easy nodes and drives the network to focus more on hard nodes. Second, to collect more comprehensive information for hard sample learning, we introduce a hierarchically contrastive scheme to sequentially force the learned node representations to involve multilevel intrinsic graph features. In this way, as the contrastive granularity goes finer, the complementary information from different levels can be uniformly encoded to boost the discrimination of hard samples and enhance the quality of the learned graph embedding. Extensive experiments on seven benchmark datasets indicate that the HCHSM performs better than other competitors on node classification and node clustering tasks. The source code of HCHSM is available at https://github.com/WxTu/HCHSM. IEEE"
Generative Subgraph Contrast for Self-Supervised Graph Representation Learning,"Contrastive learning has shown great promise in the field of graph representation learning. By manually constructing positive/negative samples, most graph contrastive learning methods rely on the vector inner product based similarity metric to distinguish the samples for graph representation. However, the handcrafted sample construction (e.g., the perturbation on the nodes or edges of the graph) may not effectively capture the intrinsic local structures of the graph. Also, the vector inner product based similarity metric cannot fully exploit the local structures of the graph to characterize the graph difference well. To this end, in this paper, we propose a novel adaptive subgraph generation based contrastive learning framework for efficient and robust self-supervised graph representation learning, and the optimal transport distance is utilized as the similarity metric between the subgraphs. It aims to generate contrastive samples by capturing the intrinsic structures of the graph and distinguish the samples based on the features and structures of subgraphs simultaneously. Specifically, for each center node, by adaptively learning relation weights to the nodes of the corresponding neighborhood, we first develop a network to generate the interpolated subgraph. We then construct the positive and negative pairs of subgraphs from the same and different nodes, respectively. Finally, we employ two types of optimal transport distances (i.e., Wasserstein distance and Gromov-Wasserstein distance) to construct the structured contrastive loss. Extensive node classification experiments on benchmark datasets verify the effectiveness of our graph contrastive learning method. Source code is available at https://github.com/yh-han/GSC.git. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG."
Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering,"Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn discriminative embeddings. Extensive experiments on four real-world datasets demonstrate the superior effectiveness of SHGP against state-of-the-art unsupervised baselines and even semi-supervised baselines. We release our source code at: https://github.com/kepsail/SHGP. © 2022 Neural information processing systems foundation. All rights reserved."
Simple Unsupervised Graph Representation Learning,"In this paper, we propose a simple unsupervised graph representation learning method to conduct effective and efficient contrastive learning. Specifically, the proposed multiplet loss explores the complementary information between the structural information and neighbor information to enlarge the inter-class variation, as well as adds an upper bound loss to achieve the finite distance between positive embeddings and anchor embeddings for reducing the intra-class variation. As a result, both enlarging inter-class variation and reducing intra-class variation result in a small generalization error, thereby obtaining an effective model. Furthermore, our method removes widely used data augmentation and discriminator from previous graph contrastive learning methods, meanwhile available to output low-dimensional embeddings, leading to an efficient model. Experimental results on various real-world datasets demonstrate the effectiveness and efficiency of our method, compared to state-of-the-art methods. The source codes are released at https://github.com/YujieMo/SUGRL. © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
Hyperspectral Band Selection Via Sparse Principal Component Analysis and Adaptive Multiple Graph Learning,"For hyperspectral image, it is a challenging task to select informative and distinctive bands due to the lack of labeled samples and massive redundancy. To address this issue, we propose a new unsupervised band selection method via Sparse Principal Component Analysis and Adaptive Multiple Graph Learning (SPCA-AMGL). Based on PCA, it proposes a Sparse PCA with L2,1 norm sparse constraint, which can effectively select the bands with high information and low correlation. In addition, an adaptive multiple graph learning is used for manifold-preserving, which ensures that the bands containing abundant spatial structure information are preserved. Specifically, it constructs multiple initial similarity graphs with different distance metrics, and then learns an adaptive graph from them. In this way, it overcomes the shortcoming of insufficient intrinsic structure of data learned from a single graph. Experimental result on Indian Pines data set proves the effectiveness and advancement of SPCA-AMGL. The source code is available at: https://github.com/ZWX0823/SPCA-AMGL. © 2022 IEEE."
Siamese Network Based Multi-Scale Self-Supervised Heterogeneous Graph Representation Learning,"Owing to label-free modeling of complex heterogeneity, self-supervised heterogeneous graph representation learning (SS-HGRL) has been widely studied in recent years. The goal of SS-HGRL is to design an unsupervised learning framework to represent complicated heterogeneous graph structures. However, based on contrastive learning, most existing methods of SS-HGRL require a large number of negative samples, which significantly increases the computation and memory costs. Furthermore, many methods cannot fully extract knowledge from a heterogeneous graph. To learn global and local information simultaneously at low time and space costs, we propose a novel Siamese Network based Multi-scale bootstrapping contrastive learning approach for Heterogeneous graphs (SNMH). Specifically, we first obtain views under the meta-path schema and the 1-hop relation type schema through dual-schema view generation. Then, we propose cross-schema and cross-view bootstrapping contrastive objectives to maximize the similarity of node representations between different schemas and views. By integrating and optimizing the above objectives, we can extract local and global information and eventually obtain the node representations for downstream tasks. To demonstrate the effectiveness of our model, we conduct experiments on several public datasets. Experimental results show that our model is superior to the state-of-the-art methods on the premise of lower time and space complexity. The source code and datasets are publicly available at https://github.com/lorisky1214/SNMH. Author"
Label propagation with structured graph learning for semi-supervised dimension reduction[Formula presented],"Graph learning has been demonstrated as one of the most effective methods for semi-supervised dimension reduction, as it can achieve label propagation between labeled and unlabeled samples to improve the feature projection performance. However, most existing methods perform this important label propagation process on the graph with sub-optimal structure, which will reduce the quality of the learned labels and thus affect the subsequent dimension reduction. To alleviate this problem, in this paper, we propose an effective Label Propagation with Structured Graph Learning (LPSGL) method for semi-supervised dimension reduction. In our model, label propagation, semi-supervised structured graph learning and dimension reduction are simultaneously performed in a unified learning framework. We propose a semi-supervised structured graph learning method to characterize the intrinsic semantic relations of samples more accurately. Further, we assign different importance scores for the given and learned labeled samples to differentiate their effects on learning the feature projection matrix. In our method, the semantic information can be propagated more effectively from labeled samples to the unlabeled samples on the learned structured graph. And a more discriminative feature projection matrix can be learned to perform the dimension reduction. An iterative optimization with the proved convergence is proposed to solve the formulated learning framework. Experiments demonstrate the state-of-the-art performance of the proposed method. The source codes and testing datasets are available at https://github.com/FWang-sdnu/LPSGL-code. © 2021 Elsevier B.V."
