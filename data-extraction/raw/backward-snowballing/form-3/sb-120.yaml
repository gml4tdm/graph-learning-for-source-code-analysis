paper-id: sb-120
pdf-id: sb-168
graphs:
  ast:
    name: ast
    description: n/a
    artefacts:
      - name: source code
        details: method
    vertex-type: ast
    edge-type: ast
    vertex-features: n/a
    edge-features: n/a
    connectivity-features:  n/a
    graph-features: n/a
    other-features: |-
      code text is used as feature
      
      summary thus far is used as feature
      
      ast is converted to sequence, where brackets are used 
      to keep structure, node name is used primarily, and lexical
      elements are added to the node name, like in "SimpleName_String"
models:
  model:
    name: n/a
    architecture-attributes:
      - encoder/decoder with 2 encoders
      - embedding layer and GRU (separate) for every sequence
      - the ast and code sequences are combined with the summary sequence (both separately) in attention layer
      - Concat, FNN
      - softmax for word prediction
tasks:
  code-summarization:
    training-objective: Given a method, generate a summary
    training-granularity: n/a
    working-objective: Given a method, generate a summary
    working-granularity: n/a
    application: Code summarization
    supervision: supervised
combinations:
  - graph: ast
    model: model
    task: code-summarization
    comments:
comments: # list