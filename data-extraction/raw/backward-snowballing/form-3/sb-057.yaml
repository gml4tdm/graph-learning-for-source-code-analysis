paper-id: sb-057
pdf-id: sb-083
graphs:
  ast:
    name: AST (binarised)
    description: n/a
    artefacts:
      - name: source code
        details: n/a
    vertex-type: ast
    edge-type: ast
    vertex-features: tokens (leaf nodes)
    edge-features: n/a
    connectivity-features: not specified
    graph-features: n/a
    other-features: source code text is used as feature
models:
  model-1:
    name: n/a
    architecture-attributes:
      - text into embedding layer w/ RNN (next term prediction)
  model-2:
    name: n/a
    architecture-attributes:
      - recursive neural network
      - starting from leaf nodes, use an auto-encoder to learn node embeddings; reconstruct child nodes, take encoder output as embedding of parent
tasks:
  next-term-prediction:
    training-objective: Given a sequence of terms, predict the next term
    training-granularity: n/a
    working-objective: n/a
    working-granularity: n/a
    application: embedding training
    supervision: self-supervised
  auto-encoding:
    training-objective: Reconstruct the two child nodes
    training-granularity: n/a
    working-objective: n/a
    working-granularity: n/a
    application: embedding training using auto-encoder (hidden layer output becomes parent node embedding)
    supervision: self-supervised
  code-clone-detection:
    training-objective: Given a pair of source code files, determine whether they are code clones by encoding both (auto-encoder style) and comparing embeddings
    training-granularity: n/a
    working-objective: Given a pair of source code files, determine whether they are code clones by encoding both (auto-encoder style) and comparing embeddings
    working-granularity: n/a
    application: code clone detection
    supervision: unsupervised
combinations:
  - graph: ast
    model: model-1 + model-2
    task: next-term-prediction + auto-encoding + code-clone-detection
    comments:
comments: # list