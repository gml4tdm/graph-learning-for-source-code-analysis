paper-id: sb-088
pdf-id: sb-171
graphs:
  ast:
    name: ast
    description: n/a
    artefacts:
      - name: source code
        details: n/a
    vertex-type: ast
    edge-type: ast
    vertex-features: node type for nonterminals, source code tokens for terminals
    edge-features: n/a
    connectivity-features: n/a
    graph-features: n/a
    other-features: |-
      The set of paths from the root to each terminal node is used for the features. 
      For each path, the nodes (type or tokens) are put into sequence. 
      
      node location information is used as feature
      
      The original code snippet is tokenised
models:
  model:
    name: n/a
    architecture-attributes:
      - Tokens (terminal node, code snippet) are encoded using as the sum of the byte pair encoding of their sub-tokens.
      - path is used as encoder input
      - Positional encoding based on node location.
      - Transformer encoder/decoder architecture.
      - original code snippet is the input for the decoder
tasks:
  tree-masked-language-modelling:
    training-objective: Given a tree with masked nodes and code snippet with correspondingly masked tokens, predict the original code snippet
    training-granularity: n/a
    working-objective: n/a
    working-granularity: n/a
    application: model pre-training
    supervision: self-supervised
  node-order-prediction:
    training-objective: Given a path with possible some swapped nodes, predict whether the order of nodes in the path is correct
    training-granularity: n/a
    working-objective: n/a
    working-granularity: n/a
    application: model pre-training
    supervision: self-supervised
combinations:
  - graph: ast
    model: model
    task: tree-masked-language-modelling + node-order-prediction
    comments:
comments: # list