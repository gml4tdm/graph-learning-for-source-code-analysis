paper-id:
pdf-id:
graphs:
  ast:
    name: ast (partial)
    description: n/a
    artefacts:
      - name: n/a
        details: generated by model
    vertex-type: ast
    edge-type: ast
    vertex-features: each node represented by an embedding, unclear how it is computed
    edge-features: n/a
    connectivity-features: not specified
    graph-features: n/a
    other-features: |-
      Description of code to be generated is used as feature (embedding unclear).
      
      Previously outputted grammar rules (sequence) are used as features.
      
      path from root to next node to expand is used as feature.
      
      Method/function scope is used as feature (nearest enclosing scope)
models:
  model:
    name: n/a
    architecture-attributes:
      - description into CNN w/ residual connections
      - grammar rules embedded using embedding layers
      - grammar rules into CNN w/ residual connections
      - ast into tree-based CNN w/ residual connections
      - Pre-order sequence of AST nodes from tree-based CNN
      - ast sequence into CNN w/ residual connections
      - max pooling over program description
      - max pooling over ast sequence output
      - embed function/method scope
      - various attention-weighted sums, using different max pooling outputs and method scope in the process.
      - concatenate all max pooling and attention layer outputs
      - MLP
      - softmax w/ beam search to determine next rule
tasks:
  code-generation:
    training-objective: Given a description, generate code
    training-granularity: sequence to graph
    working-objective: Given a description, generate code
    working-granularity: sequence to graph
    application: Code generation
    supervision: supervised
combinations:
  - graph: ast
    model: model
    task: code-generation
    comments:
comments: # list