paper-id: 196
pdf-id: 261
graphs:
  cfg: &cfg
    name: CFG
    description: n/a
    artefacts:
      - name: Source code
        details: n/a
    vertex-type:
      - name: Statement
        details: n/a
    edge-type:
      - name: Control Flow Edge
        details: Edges sometimes have labels, e.g. labelled "true" or "false" for conditionals
    vertex-features: Tokens in node encoded using word2vec
    edge-features: Encoded using word2vec (0 padded if empty)
    connectivity-features: Adjacency Matrix
    graph-features: n/a
    other-features: n/a
  pdg:
    name: PDG
    vertex-type:
      - name: Statement
        details: n/a
    edge-type:
      - name: Data Dependency Edge
        details: Annotated with variable name
      - name: Control Dependency Edge
        details: sometimes have labels, e.g. labelled "true" or "false" for conditionals
    <<: *cfg
models:
  model:
    type:
      name: n/a
      architecture: |-
        1) Node Internal Multi-head Attention (aggregate token vectors into a single vector)
            For a node V_j with tokens {v_1, v_2, ..., v_N}, compute
            i) e_i = a(W v_i)
            ii) a_i = softmax(e_i)
            iii) Do this K times to obtain K vectors a^k of length N with attention coefficients for every token
            iv) Aggregate according to 
              h_j = ELU(W_o (concat_{k = 1}^K W^ k V_j (a^k)^ T))
        2) Multi-head GAT 
        3) Edge Pooling (edge scores according to e_{ij} = W (h_i || h_j || f_{ij}) + b, where f_{ij} is the edge feature 
        4) GAT 
        5) Edge Pooling (edge scores according to e_{ij} = W (h_i || h_j || f_{ij}) + b, where f_{ij} is the edge feature 
        6) Fusion of outputs of (1), (3), and (5)
            i) To every output, apply GlobalAttention as pooling operation
                  r = \sum_{n = 1}^N softmax(h_{gate}(x_n)) \odot h_{\Theta}(x_n) (where the two h_{...} functions are neural networks)
            ii) Concatenate the 3 pooled vectors 
        7) Bidirectional LSTM 
        8) For the two input code snippets, concatenate the outputs of the two LSTMs 
        9) FNN w/ softmax
tasks:
  clone-detection:
    training-objective: Given two code samples, determine whether they are semantic clones
    training-granularity: Graph Classification (?)
    working-objective: Given two code samples, determine whether they are semantic clones
    working-granularity: Graph Classification (?)
    application: Code Clone Detection
    supervision: Supervised
combinations:
  - graph: cfg
    model: model
    task: clone-detection
    comments:
  - graph: pdg
    model: model
    task: clone-detection
    comments:
comments: # list